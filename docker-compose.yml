version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    ports:
      - "2181:2181"
    networks:
      - news_net

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"     # internal
      - "29092:29092"   # external access
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_NUM_PARTITIONS: 3
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9092", "--list"]
      interval: 20s
      timeout: 10s
      retries: 10
    networks:
      - news_net

  kafdrop:
    image: obsidiandynamics/kafdrop
    container_name: kafdrop
    depends_on:
      - kafka
    ports:
      - "9000:9000"
    environment:
      KAFKA_BROKER_CONNECT: kafka:9092
    networks:
      - news_net
  spark-master:
    image: bitnami/spark:latest
    hostname: spark-master
    container_name: spark-master
    command: ["spark-class", "org.apache.spark.deploy.master.Master"]
    ports:
      - "7070:7070"
      - "8081:8080"
    networks:
      - news_net
  

  spark-worker:
    image: bitnami/spark:latest
    hostname: spark-worker
    container_name: spark-worker
    command: ["spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]
    depends_on:
      - spark-master
    networks:
      - news_net
  kafka-producer:
    build:
        context: .
        dockerfile: ./kafka/Dockerfile
    hostname: kafka-producer
    container_name: kafka-producer
    depends_on:
      - kafka
    networks:
      - news_net
  spark_streaming:
    build:
        context: .
        dockerfile: ./spark/Dockerfile
    hostname: spark_streaming
    container_name: spark_streaming
    depends_on:
      - spark-master
      - kafka
      - cassandra
    volumes: 
      - ./cassandra:/app/cassandra
    networks:
      - news_net
networks:
    news_net: